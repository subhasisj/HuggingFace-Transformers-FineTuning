{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('pytorchgpu': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9ccd59aaf1700f687b27bd82d8fb5685c7264d29ac7c5f5c918558a7fd434024"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'d:\\\\Repositories\\\\HuggingFace-Transrformers-FineTuning\\\\PyTorch-Lightning-Fine-Tuning\\\\News-Classification-Lightning\\\\src'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pytorch-lightning in c:\\users\\subha\\appdata\\roaming\\python\\python38\\site-packages (1.0.6)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from pytorch-lightning) (4.51.0)\n",
      "Requirement already satisfied: fsspec>=0.8.0 in c:\\users\\subha\\appdata\\roaming\\python\\python38\\site-packages (from pytorch-lightning) (0.8.4)\n",
      "Requirement already satisfied: future>=0.17.1 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from pytorch-lightning) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.16.4 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from pytorch-lightning) (1.18.5)\n",
      "Requirement already satisfied: torch>=1.3 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from pytorch-lightning) (1.7.0+cu101)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from pytorch-lightning) (2.3.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from pytorch-lightning) (5.3.1)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from torch>=1.3->pytorch-lightning) (0.6)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from torch>=1.3->pytorch-lightning) (3.7.4.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.10.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.22.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.32.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (50.3.0.post20201006)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.35.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
      "\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.13.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.7.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.24.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (3.0.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\subha\\.conda\\envs\\nlp-generic\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TransformerModel.Model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 442/442 [00:00<00:00, 221kB/s]\n",
      "Downloading: 100%|██████████| 268M/268M [00:23<00:00, 11.6MB/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Model Type:<class 'transformers.modeling_distilbert.DistilBertForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "model = Model.load_from_checkpoint('./lightning_logs/version_12/checkpoints/epoch=5.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): DistilBertForSequenceClassification(\n",
       "    (distilbert): DistilBertModel(\n",
       "      (embeddings): Embeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layer): ModuleList(\n",
       "          (0): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "model.freeze()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def classify_text(text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    tokenized_text = tokenizer(text)\n",
    "\n",
    "    batch = [torch.tensor(tokenized_text['input_ids']).unsqueeze(dim=0),\n",
    "             torch.tensor(tokenized_text['attention_mask']).unsqueeze(dim=0)]\n",
    "    prediction = model(batch)\n",
    "    output  = torch.argmax(prediction[1][0])\n",
    "    return 'Fake' if  output.item() == 1 else 'Real'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fake\n"
     ]
    }
   ],
   "source": [
    "print(classify_text('Sheriff David Clarke Becomes An Internet Joke For Threatening To Poke People ‘In The Eye’'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Real\n"
     ]
    }
   ],
   "source": [
    "print(classify_text('trump wants Postal Service to charge \\'much more\\' for Amazon shipments'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}